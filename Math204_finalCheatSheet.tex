\documentclass[8pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\subsection{Simple Linear Regression}
 $$
        \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + \epsilon_i
 $$
 where $\hat{Y}_i$ are the fitted values, and
 $$
\hat{\beta}_1=\frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{S_{xy}}{S_{xx}},
 \quad\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
 $$
 $\beta_1$ is the change in the mean of $Y_i$ for a 1 unit increase in $x_i$, $\beta_0$ is the mean when $x_i= 0 $ 
 
 $S_{XX} = \sum (x_i - \bar{x} )^2$, $S_{YY} =  \sum (y_i - \bar{y} )^2$, $S_{XY} = \sum (x_i - \bar{x} )(y_i - \bar{y} )$
 
 \subsection{Estimating $\sigma^2$}
 The larger $\sigma^2$, the more dispersed the points will be around the line, i.e. less precise model.

 1) Standard deviation of $\hat{\beta}_1$: $\sigma_{\hat{\beta}_1} =\sqrt{var(\hat{\beta}_1)} = \sigma / \sqrt{S_{XX}}$
 
 2) Variance of residuals: $\hat{\sigma}^2=\frac{1}{n-2}\displaystyle\sum_{i=1}^{n}(y_i-\hat{y}_i)^2=\frac{SSE}{n-2}$
 
 3) $SSE = S_{YY} - \hat{\beta}_1 S_{XY}$
 
 4) $\hat{\sigma}_{\hat{\beta}_1} = \hat{\sigma}/\sqrt{S_{XX}}$

 \subsection{Inference about $\beta_1$}
1) When the error terms are normal, $\hat{\beta}_1 \sim \mathcal{N}(\beta_1, \sigma^2/ S_{XX})$

2) $\mathcal{H}_0 : \beta_1 = 0 \quad vs \quad \mathcal{H}_a : \beta_1 \neq 0$
$$T_{obs} = \frac{\hat{\beta}_1}{\hat{\sigma}_{\hat{\beta}_1}} = \frac{ \hat{\beta}_1}{\hat{\sigma}/\sqrt{S_{XX}}},\quad RR = \{T_{obs} > t_{\alpha/2, n-2}\}$$


3) Could get same conclusion from p-value, which illustrates the probability that our results occurred under $\mathcal{H}_0$. That is, the probability that $F>\mathcal{F}$ under $\mathcal{H}_0.$

4) Confidence interval for $\beta_1$: $\hat{\beta}_1 \pm t_{n-2, \alpha/2 } \frac{\hat{\sigma}}{\sqrt{S_{XX}}}$.

\subsection{ANOVA}
$SS_{reg} = S_{YY} - SSE = \sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i) ^2 = \sum(\hat{y}_i - \bar{y} )^2$

$T \sim t_v, \quad T^2 \sim \mathcal{F}(1, v)$
\begin{tabular}{lccccc} 
Source & df & SS & MS & $F$ & $p$-value \\
\hline Model & 1 & $\mathrm{SS}_{\text {reg }}$ & $\mathrm{MST}=\mathrm{SS}_{\mathrm{reg}}$ & $\frac{\mathrm{MST}}{\mathrm{MSE}}$ & $\operatorname{Pr}\left(F^*>F\right)$ \\
Error & $n-2$ & $\mathrm{SSE}$ & $\mathrm{MSE}=\frac{\mathrm{SSE}}{n-2}$ & & \\
\hline Total & $n-1$ & $S_{YY}$ & & &
\end{tabular}


lm summary table
\begin{itemize}
\item t-value (slope): $T_{obs} = \frac{ \hat{\beta}_1}{\hat{\sigma}_{\hat{\beta}_1}}$
\item F-statistic : $T^2_{obs} $
\item Residual std error: $\hat{\sigma}$
\end{itemize}
\subsection{Correlation}
\begin{enumerate}
\item corr(X,Y) = corr(Y,X)
\item $r = S_{XY} /\sqrt{S_{XX}S_{YY}}$ is an estimator for $\rho$ (the true pop. correlation).
\item $(1-\alpha)100\%$ confidence interval for $\rho$: 
transform $r$ to $z=0.5 \ln(\frac{1+r}{1-r})$. Build an interval:$z \pm \frac{z_{\alpha/2}}{\sqrt{n-3}}=(c_l, c_u)$, where $z_{\alpha/2}$ is from the standard Normal table. Then, the interval is
        $\left( \frac{e^{2c_L}-1}{e^{2c_L}+1},\frac{e^{2c_U}-1}{e^{2c_U}+1} \right)$
\item Coefficient of determination: $R^2 = 1 - SSE/S_{YY}$
\end{enumerate}
\subsection{Estimating response}
\begin{enumerate}
\item Mean response confidence interval: $\hat{y}_0 \pm t_{n-2, \alpha/2 } \hat{\sigma} \sqrt{ 1/n + (x_0 - \bar{x} )^2/ S_{XX}}$
\item Individual value $Y_0$ confidence interval: $\hat{y}_0 \pm t_{n-2, \alpha/2} \hat{\sigma}\sqrt{ 1+1/n + (x_0 - \bar{x} )^2/ S_{XX}}$
\end{enumerate}

\subsection{Residual Analysis} 
We estimate error terms $\epsilon_1, ... , \epsilon_n$ with residuals $\hat{\epsilon}_1,..., \hat{\epsilon}_n$, where $\hat{\epsilon}_i = y_i- \hat{y}_i$. It is recommended that we use the studentized residuals: $\hat{\epsilon}^{std}_i = \hat{\epsilon}_i / \hat{\sigma}$

1) Assumptions: $\epsilon_i$ are independent, $E(\epsilon_i) = 0, \; var(\epsilon_i) = \sigma^2, \; \epsilon_i \sim \mathcal{N} (0, \sigma^2)$

2) Check Normality with QQ plot and histogram of the studentized residuals, which have mean 0, std dev 1, all residuals should lie within 3 std deviations.

3) Check $E(\epsilon_i) = 0$ and $var(\epsilon_i) = \sigma^2$ (homoscedasticity)  by plotting studentized residuals against fitted values. Points should have equal variance and zero mean, i.e. evenly distributed.

\subsection{Polynomial Regression}
$Y_i = \beta_0 + \beta_1x_i + \beta_2x^2_i + ... + \beta_p x^p_i\epsilon_i $, not all intermediate powers need be present.

Higher-order terms are specified using the $I(\cdot) $ function in R.

1. Test that the quadratic term is zero: $H_0 :\beta_2 =0.$

2. If rejected, use  linear and quadratic terms in model.

3. If not rejected, there is no evidence that the quadratic model gives significant improvement over the linear model.

\subsection{Multiple Regression (2+ covariates)}
$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik} + \epsilon_i$

The model is linear in the parameters $(\beta_i)$, not necessarily in the covariates $(x_i)$. Same assumptions are made about the residuals. 

$\beta_j$ is the change in the mean of $Y_i$ for a 1 unit increase of $x_{ij}$ when holding all other variables constant.

1) $\hat{\sigma}^2 = (n - (K+1)) ^{-1} \sum (y_i -\hat{y}_i)^2 = SSE/(n-(K+1))$ where (K+1) is the number of coefficients $\beta_i$ in the model. 

2) Can test each coefficient individually with same hypothesis as in simple regression. In which case, we test for e.g. $\beta_j$ after adjusting for all other variables.

3) Confidence interval for $\beta_j$ : $\hat{\beta}_j \pm t_{n-(K+1), \alpha/2} \cdot \hat{\sigma}_{\hat{\beta}_j}$

4) Global Fit: $R^2_a = 1 - \frac{n-1}{n-(K+1)}\left(\frac{SSE}{S_{YY}}\right) = 1 - \frac{n-1}{n-K-1}(1-R^2) $
$\mathcal{H}_0 : \beta_1 = \beta_2 = ... = 0 \quad \mathcal{H}_a : \text{ at least one } \beta_j \neq 0$
$$F = \frac{(S_{YY} - SSE ) /K }{SSE/(n-(K+1)} = \frac{R^2/ K}{(1-R^2)/(n-(K+1))} = \frac{MSR}{MSE}$$
$$RR= \{F> \mathcal{F}_{\alpha, K, n-(K+1)} \}$$

\subsection{Interaction}
if an interaction is suspected between $X_1$ and $X_2$, we incorporate the interaction by setting
$
Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3 x_{i1} x_{i2} + \epsilon_i
	= \beta_0 + \beta_1x_{i1} +(\beta_2 + \beta_3x_{i1})x_{i2} + \epsilon_i
	= \beta_0 + (\beta_1 +\beta_3x_{i2})x_{i1} + \beta_2 x_{i2}+ \epsilon_i 
$
In the above model, a 1-unit increase in $x_2$ for a fixed $x_1$ corresponds to an estimated $\hat{\beta}_2 + \hat{\beta}_3 x_1$ increase in $Y_i$.

1) Fit the model including the covariates and interaction.

2) Conduct a global F-test with $\mathcal{H}_0 : \beta_1 = \beta_2 = \beta_3 = 0$

3) If rejected, test for an interaction by using a Student t-test to test $\mathcal{H}_0 : \beta_3 = 0$. If rejected, stop. Otherwise, re-fit the model without the interaction.

\subsection{Qualitative}
Set $Z_i = 0 \; \forall i$ for reference group and $Z_i =\left\{ \begin{matrix} 1 &\text{if condition i} \\ 0 & \text{otherwise} \end{matrix} \right.$
$$Y_i = \beta_0 + \beta_1 z_1 + \beta_2z_2 + \epsilon_i$$
Where $\hat{\beta}_0 = \mu_0$, $\hat{\beta}_1 = \mu_1 -\mu_0$, and $\hat{\beta}_2 = \mu_2- \mu_0$, and $\mathcal{H}_0 : \beta_1 = \beta_2 = 0 \iff \mathcal{H}_0 :\mu_2 = \mu_1 = \mu_0$, $\mathcal{H}_a: \geq 1 \;\beta_i\neq 0$

\textbf{Qualitative and quantitative}:

The model is $Y_i = \beta_0 + \beta_1z_i + \beta_2 x_i$. 

1) $z_i = 0$: $Y_i = \beta_0 + \beta_2x_i$

2) $z_i = 1$: $Y_i = \beta_0 + \beta_1 + \beta_2x_i$

So the slope is the same, only y-intercept changes.

Interaction: $Y_i = \beta_0 + \beta_1z_i + \beta_2 x_i + \beta_3 z_1 x_i$. Then, slopes vary:

1) $z_i = 0$: $Y_i = \beta_0 + \beta_2x_i$

2) $z_i = 1$: $Y_i = \beta_0 + \beta_1 + (\beta_2 + \beta_3)x_i$

Where $\beta_1 + \beta_3 x_i$ is the difference in $Y$ between $z_i=1$ and $z_i = 0$. Should always test for existence of an interaction.
If no evidence to reject $\mathcal{H}_0$ of no interaction, must re-fit model without interaction.
If evidence of interaction, slopes are different and interpret the results accordingly.
\subsection{Comparing Nested Models}
$M_0$ and $M_1$ are nested models if one contains a subset of the other.
$M_0 = \beta_0+...+ \beta_gx_g$, $M_1 = M_0+ \beta_{g+1}x_{g+1}+...+\beta_kx_k$
$$\mathcal{H}_0: \beta_{g+1}x_{g+1} = ...=\beta_kx_k=0 \quad \mathcal{H}_a : \text{ at least 1 } \beta_i\neq 0$$
Note: we always have $SSE_{M_0}\geq SSE_{M_1} $. 

1) $SSE_{M_0}- SSE_{M_1} $ large $\implies$ $M_1$ explains more variance than just using $M_0$.

2)$SSE_{M_0}- SSE_{M_1} $ small $\implies$ additional terms in $M_1$ don't contribute to model fit.

To determine how "large" the difference is:

$$F = \frac{(SSE_{M_0}- SSE_{M_1}) /(k-g)}{SSE_{M_1} / (n - (k+1))}, \quad RR = \{F > \mathcal{F}(\alpha,k-g, n-(k+1)) \}$$

\subsubsection{Multicollinearity} 
When two covariates in a regression analysis are highly correlated with each other. The covariates “compete” for the explanatory power in the association with the response.

\subsection{Multinomial Distribution}
One qualitative variable $C$ can take $k$ possible values $\{c_1, ... ,c_k\}$. Let $X_i$ the \# of times $c_i$ occurs. The set of $X_i$ has a multinomial distribution.
$$P(X_1 = n_1 , ... , X_k = n_k) = \frac{n!}{n_1!... n_k!} p_1^{n_1}...p_k^{n_k}$$ 
where $n_1 + ... + n_k = n$. $E(X_i) = np_i$. 
\subsubsection{Chi-square}
$\mathcal{H}_0 : p_1 = p^*_1 ,..., p_k = p^*_k \quad \mathcal{H}_a : p_i\neq p^*_i \text{ for at least one } i$

Given by Pearson's chi-square statistic : $$X^2_{obs} = \sum_{i=1}^k \frac{(n_i - np_i^*)^2}{np^*_i} = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}$$
Where $O_i:=$observed and $E_i:=$expected. Distribution of $\chi^2$ under $\mathcal{H}_0$ is $\chi^2_{(k-1)}$, $(k-1):=$(deg. of freedom). Given $\alpha$, $RR = \{X^2_{obs}> \chi^2_{\alpha, (k-1)}\}$ and $p= Pr\{\chi^2_{(k-1)} > X^2_{obs} \} $. Every expected count must be $\geq 5$ for this test.

\subsubsection{Contingency tables}

\begin{center}
\begin{tabular}{c|cccc|c}
$Y/X$ & 1 & 2 & $\hdots$ & c & Total \\ 
\hline 
1 & $n_{11}$ & $n_{12}$ & $\hdots$ & $n_{1c}$ & $n_{1 \bullet}$ \\ 
2 & $n_{21}$ & $n_{22}$ & $\hdots$ & $n_{2c}$ & $n_{2 \bullet}$ \\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ \\
r & $n_{r1}$ & $n_{r2}$ & $\hdots$ & $n_{rc}$ & $n_{r \bullet}$ \\
\hline
Total & $n_{\bullet 1}$ & $n_{\bullet 2}$ & $\hdots$ & $n_{ \bullet c}$ & $n$ 
\end{tabular}
\end{center}

$n_{j \bullet} = n_{j1} + ... + n_{jc}$, $n_{\bullet k}=n_{1k}+...+n_{rk}$. 
$n_{1\bullet} +...+ n_{r\bullet} = n_{\bullet 1} + ... + n_{\bullet c}  = n$ (sum of all entries). 

We want to test :

$\mathcal{H}_0$ : $X,Y$ independent \textit{vs} $\mathcal{H}_a$ : $X, Y$ not independent. The expected counts are given by $\hat{E}_{jk} = n \hat{p}_{j\bullet} \hat{p}_{\bullet k} = n_{j\bullet} n_{\bullet k} /n$, where $\hat{p}_{j\bullet} = n_{j\bullet} /n$, $\hat{p}_{\bullet k } = n_{\bullet k} /n$


$$ X^2 =\sum_{j= 1}^r \sum_{k=1}^c \frac{(n_{jk} - \hat{E}_{jk})^2 }{\hat{E}_{jk}}, \quad RR = \{X^2 >  \chi^2_{\alpha, (r-1)(c-1)}\} $$

Must have expected cell count $\geq 5$ for all cells, observations must be mutually independent and identically distributed.

\subsubsection{Fisher's exact test} 
If expected cell count is not $\geq 5$ for all cells.
\subsubsection{McNemar's test}
Matched pairs experiments, e.g. 
\begin{tabular}{c|ccc} 
Response 1/2 & Yes & No & Total \\
\hline Yes & $n_{11}$ & $n_{12}$ & $n_1 \bullet$ \\
No & $n_{21}$ & $n_{22}$ & $n_{2 \bullet}$ \\
\hline Total & $n_{\bullet 1}$ & $n_{\bullet 2}$ & $n$
\end{tabular}

Want to test whether the proportions are the same in both responses, i.e. $\mathcal{H}_0: p_1 = p_2$, $\mathcal{H}_a: p_1 \neq p_2$.
$$Q_M = \frac{ (n_{12} -n_{21} )^2}{n_{12} +n_{21} } , \quad \quad RR = \{ Q_M > \chi^2_{\alpha, 1}\} $$

\subsection{Non-Parametric statistics}
\subsubsection{Wilcoxon test}
To test the hypothesis that the probability distributions of both pop. are equivalent $(D_0 = D_1)$.

Conditions: independent samples, cts distributions.

1) order together the observations from both samples

2) assign a rank to each, if equality, take average of ranks.

3) take sum of ranks of each group, let $T$ the sum of sample with smaller size.

$\mathcal{H}_0: D_0 = D_1$, $\mathcal{H}_a: (T_U, T_L $ are table values),

 1) $D_1$ left of $D_2$: $RR=\{T\leq T_L\}$ if $T=T_1$, $\{T\geq T_U\}$ o.w. 
 
 2) $D_1$ right of $D_2$: $RR=\{T \geq T_U\}$ if $T=T_1$, $\{T\leq T_L\}$ o.w. 
 
 3) $D_1$ either left or right of $D_2$: $RR =\{T \leq T_L \text{ or } T \geq T_U \}$.

\subsubsection{Normal approx. of Wilcoxon}
If $n_1, n_2 \geq 10$. $Z  = \frac{T_1-(n_1 (n_1 +n_2+1 ) /2)}{\sqrt{n_1 n_2 (n_1 + n_2 +1)/ 12}}$, where $T_1$ sum of ranks corresponding to $D_1$, $n_i$ sample size. Then, $Z\sim \mathcal{N}(0,1)$ and, letting $z_\alpha$ the value from normal table:

(1) $RR = \{Z < -z_\alpha\}; p-value=Pr(Z < Z_{obs} )$

(2) $RR = \{Z > z_\alpha\}; p-value=Pr(Z > Z_{obs} )$

(3) $RR = \{|Z| > z_{\alpha/2}\}; p-value=2 \times Pr(Z > |Z_{obs}|)$

\subsubsection{Wilcoxon's signed rank test (for paired data)}
Let $X_1, ..., X_n$, $Y_1, ... , Y_n$ random samples of paired observations. $Diff_1 = X_1 - Y_1, ..., Diff_n = X_n -Y_n$

1) order absolute values of differences, take out the zeros

2) rank the differences, ties handled as usual

3) let $T_+$, $T_-$ sum of ranks of positive and negative differences

$\mathcal{H}_0: D_1= D_2$, $\mathcal{H}_a:$ same as before, with $T_0$ table value: 

1) $RR = \{ T_+ \leq T_0\}$ 2) $RR = \{ T_- \leq T_0\}$ 

3) $RR = \{ T \leq T_0 \}$ where $T=$ the smallest of $T_-$, $T_+$

\subsubsection{Large sample Wilcoxon's signed rank test}
If the number of pairs $n\geq 25$ (after excluding zeros), then $Z = \frac{T_+ -( n( n+1 ) /4 )}{\sqrt{n(n+1) (2n+1) /24}}$, $Z\sim \mathcal{N}(0,1)$, $RR$ same as 2 sections above.

\subsubsection{Kruskal-Wallis test (CRD)}
Def. (CRD): treatments are assigned randomly so that each experimental unit gets the same chance of receiving any one treatment.

KW: Rank-based non-parametric test to test difference in distribution among $\geq 2$ groups. Conditions: the K samples are random and independent, 5 or more measurements in each sample, the K probability distributions from which the samples are drawn are continuous.
$\mathcal{H}_0:$ the $k$ distributions are identical, $\mathcal{H}_a:$ at least one differs.

1) Take ranks of observations, as if they belonged to the same group.

2) Let $\overline{R}_j$ be the rank average of group $j$, $\overline{R} = (n+1)/2$ the overall avg.

Under $\mathcal{H}_0$, we expect $\overline{R}_1 \approx ... \approx \overline{R}_k$. 

$$KW = \frac{12}{ n(n+1) } \sum_{j=1}^k n_j(\overline{R}_j - \overline{R} )^2 \quad \quad RR=\{ KW>\chi^2_{\alpha, k-1} \}$$

\subsubsection{Friedman test (RBD)}
A matched set of $B$ blocks are formed with each consisting of $K$ experimental units. One experimental unit from each block is randomly assigned to each treatment.
Special case: We have B subjects, each receives all K treatments.Each subject is a block and the experimental units are the repeat assessments on the same subject.
Note: We should not give the subjects each treatment in the same order.

Condition: $B$ or $K$ $\geq 5$

1) Rank the observations within blocks.

2) $\overline{R}_j$ average of ranks within treatment j.

Total average of ranks is $K(K+1)/2$, under $\mathcal{H}_0$, we expect $\overline{R}_1 \approx ... \approx \overline{R}_K$. 
$$F_r = \frac{12 B} {K(K+1)} \sum^K_{j=1} (\overline{R}_j - \overline{R})^2 \quad \quad RR = \{ F_r > \chi^2_{\alpha, K-1}\}$$

\subsubsection{Spearman's rank correlation coefficient}
Consider $n$ mutually indep. and indentically distributed pairs of variables $(X_1, Y_1),...,(X_n, Y_n)$.

1) Rank the observations within $X : u_i$ and $Y: v_i$

2) if no ties in both, $r_s = 1- \frac{6}{n(n^2-1)}\sum_{i=1}^n (v_i-u_i)^2$

3) else, $r_s = \frac{SS_{uv}}{\sqrt{SS_{uu}SS_{vv}}}$, where $SS_{uv} = \sum (u_i -\bar{u})(v_i -\bar{v})$

A confidence interval for $\rho$ is built the same way as earlier.

\subsection{ANOVA}

To test the equality of means in $K$ populations. $\mathcal{H}_0: \mu_1 = ... = \mu_K =\mu$, $\mathcal{H}_a:$ at least one differs. Assume homoscedasticity $(\sigma_1^2 =...=\sigma_k^2)$, normally distributed response. Let $\overline{Y}_k = 1/n_k(Y_{k1}+...+ Y_{kn_k}) = \hat{\mu}_k$, $\hat{\mu} = \overline{Y}$. $SSE = \sum_{k=1}^K \sum_{i=1}^{n_k} (Y_{ki} - \overline{Y}_k)^2 = \sum_{k=1}^K (n_k -1)S_k^2$, where $S_k^2$ the sample variance in group k,  $SST = \sum_{k=1}^K n_k (\hat{\mu}_k - \hat{\mu})^2$.
$$F= \frac{MST}{MSE}= \frac{SST/(K-1)}{SSE/(n-K)}, \quad RR = \{ F> \mathcal{F}_{\alpha, K-1, n-K}\}$$

\begin{tabular}{cccccc}
 Source & $\mathrm{df}$ & SS & MS & $F$ & $p$-value \\
 \hline Treat & $K-1$ & $\mathrm{SST}$ & $\mathrm{MST}$ & $\frac{\mathrm{MST}}{\mathrm{MSE}}$ & $P\left(\mathcal{F}_{K-1, n-K}>F \right)$ \\
 Error & $n-K$ & SSE & $\mathrm{MSE}$ & & \\
\hline Total & $n-1$ &$SS_{tot}$ &$=SSE$&$+SST$ 
\end{tabular}


\subsection{Comparing means}
\subsection{Student t-test for means}
Estimates the true difference between two group means.
$100(1-\alpha) \%$ CI for the mean difference:
$$(\overline{Y}_i -\overline{Y}_j )\pm t_{\alpha/2,n_i + n_j -2} \sqrt{S^2_p/n_i +S^2_p/n_j} $$
where $S^2_p = \frac{(n_i -1 ) S^2_i +(n_j-1) S^2_j }{ n_i +n_j}$ is the pooled variance.

\subsubsection{Multiple comparisons}
Want to compare differences between groups two at a time to see how different they are. Assume equal var, normality. Conf. int for diff. in means:
$\left(\bar{Y}_i-\bar{Y}_j\right) \pm t_{\left(\alpha / 2,n-K \right)} \sqrt{MSE/ n_i+MSE / n_j}$

Bonferroni: if we want an experiment-wise error $\alpha_E$, then should take comparison-wise error $\alpha_E/C$ where C is the \# of comparisons.

Turkey's HSD: all groups of equal size, only pair-wise comp.

Scheffe's: for linear comb. of means : $\mu_1 + \mu_2 -\mu_3$
\subsubsection{ANOVA for RBD}
\begin{tabular}{lcccc}
Source & df & SS & MS & $F$ \\
\hline Treat & $K-1$ & SST & $\mathrm{MST}=\frac{\mathrm{SST}}{K-1}$ & $\frac{\mathrm{MST}}{\mathrm{MSE}}$ \\
Blocks & $B-1$ & SSB & $\mathrm{MSB}=\frac{\mathrm{SSB}}{B-1}$ & $\frac{\mathrm{MSB}}{\mathrm{MSE}}$ \\
Error & $n-K-B+1$ & SSE & $\mathrm{MSE}=\frac{\mathrm{SSE}}{n-K-B+1}$& \\
\hline
Total& $n-1$ & $\mathrm{SS}_{Tot}$
\end{tabular}

Anova $F$-test (treat) : $\mathcal{H}_0: \mu_1 = ...= \mu_k$, $\mathcal{H}_a:$ $\geq 1$ differs. 
$$F =\frac{ SST /(K-1) }{ SSE / (n-K -B +1)} = \frac{MST}{MSE}, RR = \{ F > \mathcal{F}_{\alpha, K-1, n-K-B+1}\} $$

Anova $F$-test (block) : same as above, but with the means within blocks. Replace SST with SSB, $(K-1)$ with $(B-1)$. SSB is the same as SST, but within blocks. If $\mathcal{H}_0$ rejected, proceed with MC of means. 

\subsubsection{2-way ANOVA}
To explore effect of two factors A and B on a response variable. A: J levels, B: K levels. $R$ replications for each of the $J \times K$ combinations: total observations is $n = J \times K \times R$. Let $Y_{jkr}$ the response val. for factors $A$: lvl $j$, $B$: lvl k, replication $r$. That is, $r^{th}$ rep. within treat. $(j,k)$

1) test interaction between $A$ \& $B$. If no interaction: use MC to compare pairs of treatments.

2) else test for main effect of $A$ (resp. $B$). If evidence of main effect, use MC for pairs within $A$ (resp. $B$) only.

$SST = R \sum_{j=1}^J \sum_{k=1}^K (\overline{Y}_{jk \bullet} \overline{Y})^2$,
$SSA = RK \sum_{j=1}^J (\overline{Y}_{j\bullet \bullet} -\overline{Y})^2$,
$SSB = RJ \sum_{k=1}^K (\overline{Y}_{\bullet k \bullet} -\overline{Y})^2$, $SS(AB) = SST-SSA-SSB$, where $\overline{Y}$ the overall mean. The sample variance for $(j,k)$ is $S_{jk}^2 = \frac{1}{ R-1} \sum_{r=1}^R (Y_{jkr} -\overline{Y}_{jk\bullet} )^2$, $SSE = \sum_{j,k,r =1}^{J,K,R} (Y_{jkr} - \overline{Y}_{jk \bullet} )^2 =(R-1) \sum^{J,K}_{j,k=1} S_{jk}^2$

\begin{tabular}{lcccc}
Source & df & SS & MS & $F$ \\
\hline
A & $J-1$ & SSA & $\mathrm{MSA}=\frac{\mathrm{SSA}}{J-1}$ & $\frac{\mathrm{MSA}}{\mathrm{MSE}}$ \\
B & $K-1$ & SSB & $\mathrm{MSB}=\frac{\mathrm{SSB}}{B-1}$ & $\frac{\mathrm{MSB}}{\mathrm{MSE}}$ \\
AB & $(J-1)(K-1)$ & $SS(AB)$ & $\frac{SS(AB)}{(J-1)(K-1)}$ & $\frac{MS(AB)}{MSE}$ \\
Error & $n-JK$ & SSE & $\mathrm{MSE}=\frac{\mathrm{SSE}}{n-JK}$& \\
\hline
Total& $n-1$ & $\mathrm{SS}_{Tot}$

\end{tabular}

\textbf{Interaction}

$\mathcal{H}_0 :$ Factors A and B don't interact, $\mathcal{H}_a :$ they do.
$$F = \frac{MS(AB)}{ MSE}, \quad \quad RR = \{ F > \mathcal{F}_{\alpha, (J-1)(K-1), n-JK} \} $$

\textbf{Main effect}

If $\mathcal{H}_0$ of no interaction is not rejected. $\mathcal{H}_0 :$ No difference among the J population mean responses due to factor A, 
$\mathcal{H}_a$: At least two of the means differ

$$F = \frac{SSA/(J-1) }{SSE/(n-JK)} = \frac{MSA}{MSE}, \quad RR = \{ F > \mathcal{F}_{\alpha ,J-1, n-JK} $$


\end{multicols}
\end{document}





