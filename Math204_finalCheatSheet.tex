\documentclass[8pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{MATH 204 Cheat Sheet}} \\
\end{center}
\subsection{Simple Linear Regression}
 $$
        Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
 $$
 where
 $$
\hat{\beta}_1=\frac{\sum_{i=1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}=\frac{S_{xy}}{S_{xx}}
 $$
 $$
        \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
 $$
 $\beta_1$ is the change in the mean of $Y_i$ for a 1 unit increase in $x_i$, $\beta_0$ is the mean when $x_i= 0 $ 
 
 $S_{XX} = \sum (x_i - \bar{x} )^2$, $S_{YY} =  \sum (y_i - \bar{y} )^2$, $S_{XY} = \sum (x_i - \bar{x} )(y_i - \bar{y} )$
 
 \subsection{Estimating $\sigma^2$}
 \begin{enumerate}
 \item Standard deviation of $\hat{\beta}_1$: $\sigma_{\hat{\beta}_1} =\sqrt{var(\hat{\beta}_1)} = \sigma / \sqrt{S_{XX}}$
 \item Variance of residuals: $\hat{\sigma}^2=\frac{1}{n-2}\displaystyle\sum_{i=1}^{n}(y_i-\hat{y}_i)^2=\frac{SSE}{n-2}$
 \item $SSE = S_{YY} - \hat{\beta}_1 S_{XY}$
 \item $\hat{\sigma}_{\hat{\beta}_1} = \hat{\sigma}/\sqrt{S_{XX}}$
 \end{enumerate}
 \subsection{Inference about $\beta_1$}
\begin{enumerate}
\item When the error terms are normal, $\hat{\beta}_1 \sim \mathcal{N}(\beta_1, \sigma^2/ S_{XX})$
\item $T = \frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma}/S_{XX}} \sim t_{n-2}$

$$\mathcal{H}_0 : \beta_1 = 0 \quad vs \quad \mathcal{H}_a : \beta_1 \neq 0$$
$T_{obs} = \frac{\hat{\beta}_1}{\hat{\sigma}_{\hat{\beta}_1}} = \frac{ \hat{\beta}_1}{\hat{\sigma}/\sqrt{S_{XX}}}$

Compare $T_{obs}$ with the student distribution $t_{n-2, \alpha/2}$ to get RR.
\item Could get same conclusion from p-value, which illustrates the probability that our results occurred under $\mathcal{H}_0$.

\item Confidence interval for $\beta_1$: $\hat{\beta}_1 \pm t_{n-2, \alpha/2 } \frac{\hat{\sigma}}{\sqrt{S_{XX}}}$.
\end{enumerate}  
\subsection{ANOVA}
\begin{enumerate}
\item $SS_{reg} = S_{YY} - SSE = \sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i) ^2 = \sum(\hat{y}_i - \bar{y} )^2$
\item $T \sim t_v, \quad T^2 \sim \mathcal{F}(1, v)$, where the latter is the Fisher-Snedecor dis.
\item ANOVA table guide:
\begin{itemize}
	\item (X, Sum Sq) = $SS_{reg}$
	\item (Residuals, Sum Sq) = SSE
	\item (Residuals, Df) = $n-2$
\end{itemize}
\item lm summary table
\begin{itemize}
\item t-value (slope): $T_{obs} = \frac{ \hat{\beta}_1}{\hat{\sigma}_{\hat{\beta}_1}}$
\item F-statistic : $T^2_{obs} $
\item Residual std error: $\hat{\sigma}$
\end{itemize}
\end{enumerate}
\subsection{Correlation}
\begin{enumerate}
\item corr(X,Y) = corr(Y,X)
\item $r = S_{XY} /\sqrt{S_{XX}S_{YY}}$ is an estimator for $\rho$ (the true pop. correlation).
\item $(1-\alpha)100\%$ confidence interval for $\rho$: 
transform $r$ to $z=0.5 \ln(\frac{1+r}{1-r})$. Build an interval:$z \pm \frac{z_{\alpha/2}}{\sqrt{n-3}}=(c_l, c_u)$, where $z_{\alpha/2}$ is from the standard Normal table. Then, the interval is
        $\left( \frac{e^{2c_L}-1}{e^{2c_L}+1},\frac{e^{2c_U}-1}{e^{2c_U}+1} \right)$
\item Coefficient of determination: $R^2 = 1 - SSE/S_{YY}$
\end{enumerate}
\subsection{Estimating response}
\begin{enumerate}
\item Mean response confidence interval: $\hat{y}_0 \pm t_{n-2, \alpha/2 } \hat{\sigma} \sqrt{ 1/n + (x_0 - \bar{x} )^2/ S_{XX}}$
\item Individual value $Y_0$ confidence interval: $\hat{y}_0 \pm t_{n-2, \alpha/2} \hat{\sigma}\sqrt{ 1+1/n + (x_0 - \bar{x} )^2/ S_{XX}}$
\end{enumerate}

\subsection{Residual Analysis} 
\begin{enumerate}
\item Assumptions: $\epsilon_i$ are independent, $E(\epsilon_i) = 0, \; var(\epsilon_i) = \sigma^2, \; \epsilon_i \sim \mathcal{N} (0, \sigma^2)$
\item Check Normality with QQ plot and histogram of the studentized residuals, which have mean 0, all residuals should lie within 3 std deviations.
\item Check $E(\epsilon_i) = 0$ by plotting studentized residuals against fitted values. Points should have equal variance and zero mean, i.e. evenly distributed.
\end{enumerate}

\subsection{Polynomial Regression}
$Y_i = \beta_0 + \beta_1x_i + \beta_2x^2_i + ... + \beta_p x^p_i\epsilon_i $, not all intermediate powers need be present.

Higher-order terms are specified using the $I(\cdot) $ function in R.

1. Test that the quadratic term is zero: $H_0 :\beta_2 =0.$

2. If rejected, use  linear and quadratic terms in model.

3. If not rejected, there is no evidence that the quadratic model gives significant improvement over the linear model.

\subsection{Multiple Regression (2+ covariates)}
$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik} + \epsilon_i$

The model is linear in the parameters $(\beta_i)$, not necessarily in the covariates $(x_i)$. Same assumptions are made about the residuals. 

$\beta_j$ is the change in the mean of $Y_i$ for a 1 unit increase of $x_{ij}$ when holding all other variables constant.

\begin{enumerate}
\item $\hat{\sigma}^2 = (n - (K+1)) ^{-1} \sum (y_i -\hat{y}_i)^2 = SSE/(n-(K+1))$ where (K+1) is the number of coefficients $\beta_i$ in the model. 
\item Can test each coefficient individually with same hypothesis as in simple regression. In which case, we test for e.g. $\beta_j$ after adjusting for all other variables.
\item Confidence interval for $\beta_j$ : $\hat{\beta}_j \pm t_{n-(K+1), \alpha/2} \cdot \hat{\sigma}_{\hat{\beta}_j}$
\item Global Fit

$R^2_a = 1 - \frac{n-1}{n-(K+1)}\left(\frac{SSE}{S_{YY}}\right) = 1 - \frac{n-1}{n-K-1}(1-R^2) $

e.g. if $R^2_a = 0.80$, then we say that the model explains 80\% of the variance in Y.

\textbf{Overall hypothesis:}
$$\mathcal{H}_0 : \beta_1 = \beta_2 = ... = 0 \quad \mathcal{H}_a : \text{ at least one } \beta_j \neq 0$$
$F = \frac{(S_{YY} - SSE ) /K }{SSE/(n-(K+1)} = \frac{R^2/ K}{(1-R^2)/(n-(K+1))}$

$\mathcal{H}_0$ is rejected for $F> \mathcal{F}_{\alpha, K, n-(K+1)}$.

\end{enumerate}

\end{multicols}
\end{document}








